{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd820fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path \n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5981d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"./tig_dataset\"\n",
    "TOKENIZER_PATH = \"./tokenizers/Tig_unigram_16000\"\n",
    "SAVE_PATH = \"./saved_models/bert\"\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bde48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import BERT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cfa058",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 8\n",
    "BLOCK_SIZE = 128             # max tokens per input\n",
    "NUM_EPOCHS = 3\n",
    "LR = 3e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_ACCUM_STEPS = 2\n",
    "LOG_INTERVAL = 50\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b40ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Tokenizer & Model\n",
    "# ================================================\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "BERT_CONFIG.num_labels = 2  # modify for your classification task\n",
    "\n",
    "model = BertForSequenceClassification(BERT_CONFIG).to(DEVICE)\n",
    "print(\"Tokenizer and model loaded ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a70284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dataset\n",
    "# ================================================\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Custom dataset for BERT training.\"\"\"\n",
    "    def __init__(self, folder_path, tokenizer, block_size=128):\n",
    "        self.file_paths = list(Path(folder_path).rglob(\"*.txt\"))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.samples = []\n",
    "        # Pre-tokenize all files (optional for small datasets)\n",
    "        for file_path in self.file_paths:\n",
    "            text = Path(file_path).read_text(encoding=\"utf-8\").strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            tokens = self.tokenizer(text, truncation=True, max_length=block_size, padding=\"max_length\")\n",
    "            self.samples.append((tokens['input_ids'], tokens['attention_mask'], 0))  # replace 0 with your label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids, attention_mask, label = self.samples[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(DATA_PATH, tokenizer, block_size=BLOCK_SIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa32815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Optimizer & Scheduler\n",
    "# ================================================\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training Loop\n",
    "# ================================================\n",
    "def train_model(model, dataloader, optimizer, scheduler, num_epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for step, batch in enumerate(pbar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / GRAD_ACCUM_STEPS\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "            if step % LOG_INTERVAL == 0 and step > 0:\n",
    "                avg_loss = running_loss / LOG_INTERVAL\n",
    "                pbar.set_postfix(loss=f\"{avg_loss:.4f}\", ppl=f\"{math.exp(avg_loss):.2f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        ckpt_path = os.path.join(MODEL_OUTPUT, f\"epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"✅ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "train_model(model, train_loader, optimizer, scheduler, NUM_EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Evaluation\n",
    "# ================================================\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds, labels_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            preds.extend(pred.cpu().tolist())\n",
    "            labels_list.extend(labels.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(labels_list, preds)\n",
    "    f1 = f1_score(labels_list, preds, average=\"weighted\")\n",
    "    print(\"=== Evaluation Results ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(labels_list, preds))\n",
    "\n",
    "# Use the same train_loader or a separate validation loader\n",
    "evaluate_model(model, train_loader, DEVICE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
