{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf92b4c7",
   "metadata": {},
   "source": [
    "# GPT Training Notebook\n",
    "**Purpose:** training loop and orchestration notebook using a custom `GPTModel`, `tokenizer`, `dataset`, and `config`.  \n",
    "Features: streaming data support, FP16/AMP, gradient accumulation, checkpointing, TensorBoard logging, evaluation metrics (loss, perplexity, accuracy, tokens/sec, GPU usage), and generation utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a686fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader,IterableDataset\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path \n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8453aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training config: {'vocab_size': None, 'context_len': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'dropout_rate': 0.1, 'qkv_bias': False, 'batch_size': 2, 'grad_accum_steps': 8, 'learning_rate': 0.0003, 'weight_decay': 0.1, 'max_iters': 200000, 'eval_interval': 2000, 'save_interval': 5000, 'lr_warmup_iters': 2000, 'max_grad_norm': 1.0, 'use_fp16': True, 'context_stride': 1, 'num_epochs': 3}\n"
     ]
    }
   ],
   "source": [
    "from config import train_config\n",
    "print(f\"training config: {train_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caf43dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"./tig_dataset\"\n",
    "TOKENIZER_PATH = \"./tokenizers/Tig_unigram_16000\"\n",
    "SAVE_PATH = \"./saved_models/gpt\"\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7d5b63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded âœ…\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2. Load Tokenizer\n",
    "# ================================================\n",
    "from transformers import PreTrainedTokenizerFast, GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT usually has no pad token, use EOS\n",
    "\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_PATH)\n",
    "print(\"Tokenizer loaded âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "142553d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT model loaded âœ…\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 4. Load Your GPT Model\n",
    "# ================================================\n",
    "from gpt_model import GPTModel  # import your model class\n",
    "\n",
    "from config import GPT_CONFIG  # import your config dictionary\n",
    "GPT_CONFIG[\"vocab_size\"] = tokenizer.vocab_size  # update vocab size based on tokenizer\n",
    "\n",
    "model = GPTModel(GPT_CONFIG).to(device)\n",
    "print(\"GPT model loaded âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dc89100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50257\n",
      "Model embedding size: 50257\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Model embedding size:\", model.tok_emb.num_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "170a3e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1 text files from ./tig_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1204933 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenized full dataset: 1204933 tokens\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for file_path in Path(DATA_PATH).rglob(\"*eng.txt\"):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts.append(f.read())\n",
    "full_text = \"\\n\".join(texts)\n",
    "print(f\"âœ… Loaded {len(texts)} text files from {DATA_PATH}\")\n",
    "\n",
    "# Tokenize entire corpus once\n",
    "token_ids = tokenizer(full_text, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "print(f\"âœ… Tokenized full dataset: {token_ids.shape[0]} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a3d85a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset ready with 1176 sequences\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokens, context_len=1024):\n",
    "        self.tokens = tokens\n",
    "        self.context_len = context_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.tokens) - 1) // self.context_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.context_len\n",
    "        end = start + self.context_len\n",
    "        x = self.tokens[start:end]\n",
    "        y = self.tokens[start+1:end+1]\n",
    "        return x, y\n",
    "\n",
    "dataset = TokenDataset(token_ids, context_len=train_config[\"context_len\"])\n",
    "dataloader = DataLoader(dataset, batch_size=train_config[\"batch_size\"], shuffle=True)\n",
    "print(f\"âœ… Dataset ready with {len(dataset)} sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1105fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=train_config[\"learning_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95c2eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = train_config[\"num_epochs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5732052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Dataset class for sequential token chunks\n",
    "# -------------------------------\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, token_ids, context):\n",
    "        self.token_ids = token_ids\n",
    "        self.context = context\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.token_ids) - self.context)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.token_ids[idx: idx + self.context]\n",
    "        target_ids = self.token_ids[idx + 1: idx + 1 + self.context]\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "# -------------------------------\n",
    "# Perplexity helper\n",
    "# -------------------------------\n",
    "def compute_perplexity(loss):\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14d04ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/588 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(train_config[\"num_epochs\"]):\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{train_config['num_epochs']}\")\n",
    "        for step, (inp, tgt) in enumerate(pbar):\n",
    "            inp, tgt = inp.to(device), tgt.to(device)\n",
    "\n",
    "            logits, loss = model(inp, targets=tgt)\n",
    "            loss = loss / train_config[\"gradient_accumulation_steps\"]\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % train_config[\"gradient_accumulation_steps\"] == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * train_config[\"gradient_accumulation_steps\"]\n",
    "            if step % 50 == 0 and step > 0:\n",
    "                avg_loss = running_loss / 50\n",
    "                pbar.set_postfix(loss=f\"{avg_loss:.4f}\", ppl=f\"{compute_perplexity(avg_loss):.2f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        ckpt_path = f\"{SAVE_PATH}/epoch_{epoch+1}.pth\"\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"âœ… Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# TRAINING CALL\n",
    "# -------------------------------\n",
    "\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "train()\n",
    "print(\"âœ… Training complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990be63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# TEXT GENERATION FUNCTION\n",
    "# -------------------------------\n",
    "def generate_text(prompt, max_tokens=100):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "    generated = model.generate(input_ids, max_new_tokens=max_tokens)\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "# Example generation\n",
    "prompt = \"Once upon a time\"\n",
    "print(\"ðŸ“ Generated text:\")\n",
    "print(generate_text(prompt, max_tokens=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47aa0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, token_ids, device, num_epochs=num_epochs,\n",
    "      batch_size=train_config[\"batch_size\"], context=train_config[\"context_len\"],\n",
    "      lr=train_config[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "prompt = \"Once upon a time\"\n",
    "prompt_ids = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "# generate 100 new tokens\n",
    "generated_ids = model.generate(prompt_ids, max_new_tokens=100)\n",
    "generated_text = tokenizer.decode(generated_ids[0].tolist())\n",
    "print(\"âœ… Generated text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"path/to/epoch_3.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e4806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "gradient_accumulation_steps = 4\n",
    "log_interval = 50\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    return math.exp(loss)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for step, (inp, tgt) in enumerate(pbar):\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        logits = model(inp)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), tgt.view(-1), ignore_index=-100)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * gradient_accumulation_steps\n",
    "        if step % log_interval == 0 and step > 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            pbar.set_postfix(loss=f\"{avg_loss:.3f}\", ppl=f\"{compute_perplexity(avg_loss):.2f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    ckpt_path = f\"checkpoints/epoch_{epoch+1}.pt\"\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    print(f\"âœ… Saved checkpoint: {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc2ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # ================================================\n",
    "# # 6. Training Loop\n",
    "# # ================================================\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "#     for step, (inp, tgt) in enumerate(pbar):\n",
    "#         inp, tgt = inp.to(device), tgt.to(device)\n",
    "#         logits = model(inp)\n",
    "#         loss = F.cross_entropy(logits.view(-1, logits.size(-1)), tgt.view(-1), ignore_index=-100)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         if step % log_interval == 0 and step > 0:\n",
    "#             avg_loss = running_loss / log_interval\n",
    "#             perplexity = compute_perplexity(avg_loss)\n",
    "#             pbar.set_postfix(loss=f\"{avg_loss:.3f}\", ppl=f\"{perplexity:.2f}\")\n",
    "#             running_loss = 0.0\n",
    "\n",
    "#     # Save checkpoint\n",
    "#     ckpt_path = f\"checkpoints/epoch_{epoch+1}.pt\"\n",
    "#     torch.save(model.state_dict(), ckpt_path)\n",
    "#     print(f\"âœ… Saved checkpoint: {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 7. Evaluation (Perplexity)\n",
    "# ================================================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    for inp, tgt in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        logits = model(inp)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            tgt.view(-1),\n",
    "            ignore_index=-100,\n",
    "            reduction=\"sum\"\n",
    "        )\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += (tgt != -100).sum().item()\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "val_loss = evaluate(model, train_loader)\n",
    "print(f\"âœ… Validation Perplexity: {math.exp(val_loss):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
