{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf92b4c7",
   "metadata": {},
   "source": [
    "# GPT Training Notebook\n",
    "**Purpose:** training loop and orchestration notebook using a custom `GPTModel`, `tokenizer`, `dataset`, and `config`.  \n",
    "Features: streaming data support, FP16/AMP, gradient accumulation, checkpointing, TensorBoard logging, evaluation metrics (loss, perplexity, accuracy, tokens/sec, GPU usage), and generation utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a686fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pr401\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader,IterableDataset\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path \n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8453aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training config: {'vocab_size': None, 'context_len': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'dropout_rate': 0.1, 'qkv_bias': False, 'batch_size': 4, 'grad_accum_steps': 8, 'learning_rate': 0.0003, 'weight_decay': 0.1, 'max_iters': 200000, 'eval_interval': 2000, 'save_interval': 5000, 'lr_warmup_iters': 2000, 'max_grad_norm': 1.0, 'use_fp16': True, 'context_stride': 1}\n"
     ]
    }
   ],
   "source": [
    "from config import train_config\n",
    "print(f\"training config: {train_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf43dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"./tig_dataset\"\n",
    "TOKENIZER_PATH = \"./tokenizers/Tig_unigram_16000\"\n",
    "SAVE_PATH = \"./saved_models/gpt\"\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a8b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded ✅\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2. Load Tokenizer\n",
    "# ================================================\n",
    "from transformers import PreTrainedTokenizerFast, GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT usually has no pad token, use EOS\n",
    "\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_PATH)\n",
    "print(\"Tokenizer loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d85a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 text files.\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 3. Prepare Dataset\n",
    "# ================================================\n",
    "class TextFolderDataset(Dataset):\n",
    "    \"\"\"Streams and tokenizes large text datasets efficiently.\"\"\"\n",
    "    def __init__(self, folder_path, tokenizer, block_size=1024):\n",
    "        self.file_paths = list(Path(folder_path).rglob(\"*.txt\"))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        print(f\"Found {len(self.file_paths)} text files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = Path(self.file_paths[idx]).read_text(encoding=\"utf-8\")\n",
    "        tokens = self.tokenizer.encode(text, truncation=True, max_length=self.block_size, return_tensors=\"pt\")\n",
    "        tokens = tokens.squeeze(0)\n",
    "        input_ids = tokens[:-1]\n",
    "        target_ids = tokens[1:]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"Pads variable-length token sequences in a batch.\"\"\"\n",
    "    input_ids, target_ids = zip(*batch)\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    target_ids = torch.nn.utils.rnn.pad_sequence(target_ids, batch_first=True, padding_value=-100)\n",
    "    return input_ids, target_ids\n",
    "\n",
    "# train_data = TextFolderDataset(DATA_PATH, tokenizer)\n",
    "# train_loader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "335ae0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingTextDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Streams large text datasets in chunks for memory-efficient GPT training.\n",
    "    Each chunk becomes a block of tokens of `block_size`.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder_path, tokenizer, block_size=1024, chunk_size=32_768):\n",
    "        self.file_paths = list(Path(folder_path).rglob(\"*.txt\"))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def _read_file_in_chunks(self, file_path):\n",
    "        \"\"\"Yield text chunks from a file.\"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            while True:\n",
    "                chunk = f.read(self.chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                yield chunk\n",
    "\n",
    "    def _tokenize_chunk(self, chunk):\n",
    "        \"\"\"Tokenize chunk and split into block_size sequences.\"\"\"\n",
    "        tokens = self.tokenizer.encode(chunk, truncation=False, add_special_tokens=False)\n",
    "        blocks = [tokens[i:i+self.block_size] for i in range(0, len(tokens), self.block_size)]\n",
    "        return blocks\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_path in self.file_paths:\n",
    "            for chunk in self._read_file_in_chunks(file_path):\n",
    "                for block in self._tokenize_chunk(chunk):\n",
    "                    if len(block) < 2:\n",
    "                        continue\n",
    "                    tokens = torch.tensor(block, dtype=torch.long)\n",
    "                    input_ids = tokens[:-1]\n",
    "                    target_ids = tokens[1:]\n",
    "                    yield input_ids, target_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f083cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    input_ids, target_ids = zip(*batch)\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    target_ids = torch.nn.utils.rnn.pad_sequence(target_ids, batch_first=True, padding_value=-100)\n",
    "    return input_ids, target_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b5e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and DataLoader ready ✅\n"
     ]
    }
   ],
   "source": [
    "train_dataset = StreamingTextDataset(DATA_PATH, tokenizer, block_size=1024, chunk_size=32_768)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,           # small batches to avoid OOM\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=2\n",
    ")\n",
    "print(\"Dataset and DataLoader ready ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6299f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT model loaded ✅\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 4. Load Your GPT Model\n",
    "# ================================================\n",
    "from gpt_model import GPTModel  # import your model class\n",
    "\n",
    "from config import GPT_CONFIG  # import your config dictionary\n",
    "\n",
    "model = GPTModel(GPT_CONFIG).to(device)\n",
    "print(\"GPT model loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e4806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "gradient_accumulation_steps = 4\n",
    "log_interval = 50\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    return math.exp(loss)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for step, (inp, tgt) in enumerate(pbar):\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        logits = model(inp)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), tgt.view(-1), ignore_index=-100)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * gradient_accumulation_steps\n",
    "        if step % log_interval == 0 and step > 0:\n",
    "            avg_loss = running_loss / log_interval\n",
    "            pbar.set_postfix(loss=f\"{avg_loss:.3f}\", ppl=f\"{compute_perplexity(avg_loss):.2f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    ckpt_path = f\"checkpoints/epoch_{epoch+1}.pt\"\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    print(f\"✅ Saved checkpoint: {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc2ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # ================================================\n",
    "# # 6. Training Loop\n",
    "# # ================================================\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "#     for step, (inp, tgt) in enumerate(pbar):\n",
    "#         inp, tgt = inp.to(device), tgt.to(device)\n",
    "#         logits = model(inp)\n",
    "#         loss = F.cross_entropy(logits.view(-1, logits.size(-1)), tgt.view(-1), ignore_index=-100)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         if step % log_interval == 0 and step > 0:\n",
    "#             avg_loss = running_loss / log_interval\n",
    "#             perplexity = compute_perplexity(avg_loss)\n",
    "#             pbar.set_postfix(loss=f\"{avg_loss:.3f}\", ppl=f\"{perplexity:.2f}\")\n",
    "#             running_loss = 0.0\n",
    "\n",
    "#     # Save checkpoint\n",
    "#     ckpt_path = f\"checkpoints/epoch_{epoch+1}.pt\"\n",
    "#     torch.save(model.state_dict(), ckpt_path)\n",
    "#     print(f\"✅ Saved checkpoint: {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 7. Evaluation (Perplexity)\n",
    "# ================================================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    for inp, tgt in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        logits = model(inp)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            tgt.view(-1),\n",
    "            ignore_index=-100,\n",
    "            reduction=\"sum\"\n",
    "        )\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += (tgt != -100).sum().item()\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "val_loss = evaluate(model, train_loader)\n",
    "print(f\"✅ Validation Perplexity: {math.exp(val_loss):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
