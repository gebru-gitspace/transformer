{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# core\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# data & tokenization\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# utils\n",
    "from typing import List, Tuple\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf43dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"./tig_dataset\"\n",
    "TOKENIZER_DIR = \"./tokenizers/Tig_unigram_16000\"\n",
    "SAVE_DIR = \"./saved_models/gpt_small\"\n",
    "\n",
    "# Training hyperparams\n",
    "VOCAB_SIZE = 16000\n",
    "BLOCK_SIZE = 128            # sequence length\n",
    "BATCH_SIZE = 8              # per step\n",
    "EPOCHS = 3\n",
    "LR = 3e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model architecture (small for quick experiments)\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8\n",
    "EMBED_DIM = 256\n",
    "FF_DIM = EMBED_DIM * 4\n",
    "\n",
    "os.makedirs(TOKENIZER_DIR, exist_ok=True)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3076ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer (trained above or existing)\n",
    "from tokenizers import Tokenizer as TokenizersTokenizer\n",
    "\n",
    "tokenizer_path = os.path.join(TOKENIZER_DIR, \"tokenizer.json\")\n",
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = TokenizersTokenizer.from_file(tokenizer_path)\n",
    "    # helper wrappers\n",
    "    def encode_text(s: str) -> List[int]:\n",
    "        return tokenizer.encode(s).ids\n",
    "    def decode_ids(ids: List[int]) -> str:\n",
    "        return tokenizer.decode(ids)\n",
    "    print(\"Loaded tokenizer from\", tokenizer_path)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Tokenizer not found at \" + tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3324eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect text files\n",
    "text_files = []\n",
    "for p in Path(DATA_DIR).glob(\"**/*.txt\"):\n",
    "    text_files.append(str(p))\n",
    "\n",
    "if len(text_files) == 0:\n",
    "    print(\"No text files found in\", DATA_DIR, \"- skip tokenizer training and load existing tokenizer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset: load texts and convert to long stream of token ids\n",
    "# This builds a dataset of contiguous token-id blocks of length BLOCK_SIZE for causal LM.\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, files: List[str], encode_fn, block_size: int):\n",
    "        self.ids = []\n",
    "        for f in files:\n",
    "            with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "                for line in fh:\n",
    "                    s = line.strip()\n",
    "                    if not s:\n",
    "                        continue\n",
    "                    ids = encode_fn(s)\n",
    "                    # append BOS/EOS optionally:\n",
    "                    # here we keep as is, but you can add special tokens if your tokenizer has them\n",
    "                    self.ids.extend(ids + [tokenizer.token_to_id(\"<eos>\")] if tokenizer.token_to_id(\"<eos>\") is not None else ids)\n",
    "        # convert continuous stream into fixed-length blocks\n",
    "        self.block_size = block_size\n",
    "        total_tokens = len(self.ids)\n",
    "        self.num_blocks = total_tokens // block_size\n",
    "    def __len__(self):\n",
    "        return self.num_blocks\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.block_size\n",
    "        chunk = self.ids[start:start + self.block_size]\n",
    "        return torch.tensor(chunk, dtype=torch.long)\n",
    "\n",
    "# build dataset\n",
    "files = text_files\n",
    "if len(files) == 0:\n",
    "    # demo fallback: tiny built-in dataset from HF\n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
    "    tmpfile = os.path.join(DATA_DIR, \"tmp_corpus.txt\")\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    with open(tmpfile, \"w\", encoding=\"utf-8\") as fh:\n",
    "        for r in ds:\n",
    "            if r[\"text\"].strip():\n",
    "                fh.write(r[\"text\"].strip() + \"\\n\")\n",
    "    files = [tmpfile]\n",
    "\n",
    "dataset = TextDataset(files, encode_text, BLOCK_SIZE)\n",
    "print(\"Dataset blocks:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c72155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader & collate function (creates inputs and targets)\n",
    "def collate_fn(batch):\n",
    "    # batch: list of tensors each length BLOCK_SIZE\n",
    "    batch = torch.stack(batch)  # (B, block)\n",
    "    # inputs x and targets y shifted by 1\n",
    "    x = batch[:, :-1].contiguous()\n",
    "    y = batch[:, 1:].contiguous()\n",
    "    return x, y\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT model implementation (small decoder-only transformer)\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, attn_dropout=0.0, resid_dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "        self.qkv = nn.Linear(n_embd, 3 * n_embd)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.resid_dropout = nn.Dropout(resid_dropout)\n",
    "\n",
    "        # causal mask is created in forward for current sequence length\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv(x)  # (B,T,3C)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "        # reshape for multi-head\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hd)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * self.scale  # (B, nh, T, T)\n",
    "        # causal mask\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "        att = att.masked_fill(mask.unsqueeze(0).unsqueeze(0), float(\"-inf\"))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        out = att @ v  # (B, nh, T, hd)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.resid_dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, ff_hidden_dim, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, attn_dropout=pdrop, resid_dropout=pdrop)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, ff_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_hidden_dim, n_embd),\n",
    "            nn.Dropout(pdrop)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTSmall(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layers, n_heads, n_embd, ff_hidden):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.blocks = nn.ModuleList([GPTBlock(n_embd, n_heads, ff_hidden) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx: (B, T)\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.pos_emb.size(1)\n",
    "        tok = self.tok_emb(idx)  # (B,T,emb)\n",
    "        x = tok + self.pos_emb[:, :t, :]\n",
    "        x = self.drop(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B,T,vocab)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92570a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model, optimizer, loss\n",
    "vocab_size_actual = VOCAB_SIZE  # if tokenizer has different size, set accordingly\n",
    "# attempt to get tokenizer vocab size if available\n",
    "try:\n",
    "    vocab_size_actual = len(tokenizer.get_vocab())\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "model = GPTSmall(vocab_size=vocab_size_actual, block_size=BLOCK_SIZE - 1, n_layers=N_LAYERS,\n",
    "                 n_heads=N_HEADS, n_embd=EMBED_DIM, ff_hidden=FF_DIM).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0873da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling / generation (greedy and top-k)\n",
    "@torch.no_grad()\n",
    "def generate(model, start_ids: List[int], max_new_tokens=50, temperature=1.0, top_k=None):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(start_ids, dtype=torch.long, device=DEVICE).unsqueeze(0)  # (1, T)\n",
    "    for _ in range(max_new_tokens):\n",
    "        t = idx.size(1)\n",
    "        # crop context if larger than model block size\n",
    "        if t > model.pos_emb.size(1):\n",
    "            idx_cond = idx[:, -model.pos_emb.size(1):]\n",
    "        else:\n",
    "            idx_cond = idx\n",
    "        logits = model(idx_cond)  # (1, T, V)\n",
    "        logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)  # (1, V)\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            minv = v[:, -1].unsqueeze(-1)\n",
    "            logits = torch.where(logits < minv, torch.tensor(-1e10, device=logits.device), logits)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx[0].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example generate (use your tokenizer to get start ids)\n",
    "prompt = \"ሰላም\"\n",
    "start_ids = encode_text(prompt)\n",
    "gen_ids = generate(model, start_ids, max_new_tokens=50, temperature=1.0, top_k=50)\n",
    "print(\"Generated IDs:\", gen_ids)\n",
    "print(\"Decoded:\", decode_ids(gen_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for reuse (HuggingFace-compatible minimal)\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": {\n",
    "        \"vocab_size\": vocab_size_actual,\n",
    "        \"block_size\": BLOCK_SIZE - 1,\n",
    "        \"n_layers\": N_LAYERS,\n",
    "        \"n_heads\": N_HEADS,\n",
    "        \"n_embd\": EMBED_DIM,\n",
    "        \"ff_dim\": FF_DIM\n",
    "    }\n",
    "}, os.path.join(SAVE_DIR, \"gpt_small_final.pt\"))\n",
    "\n",
    "# tokenizer already saved earlier as tokenizer.json\n",
    "print(\"Saved final model and tokenizer (if tokenizer was trained).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
