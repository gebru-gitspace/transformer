{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\projects\\aman\\transformer\\tig_dataset\n"
     ]
    }
   ],
   "source": [
    "# Path to the dataset folder one level above the current working directory\n",
    "DATA_PATH = os.path.join(\".\", \"tig_dataset\")\n",
    "print(os.path.abspath(DATA_PATH))  # Shows the full path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_List(data_path):\n",
    "    \"\"\"\n",
    "    Reads our trainig data (list of files that have the training text)\n",
    "    \n",
    "    Returns list of list sentences for each file\n",
    "    \"\"\"\n",
    "    list_files=os.listdir(data_path)\n",
    "    dataset_list=[]\n",
    "    for file_path in list_files:\n",
    "        if file_path.endswith(\".txt\"):\n",
    "            with open(os.path.join(data_path,file_path), 'r') as file:\n",
    "                lines=file.readlines()\n",
    "            sentences=[]\n",
    "            for line in lines:\n",
    "                if line!=\"\\n\":\n",
    "                    sentences.append(line)\n",
    "                else:\n",
    "                    dataset_list.append(sentences)\n",
    "                    sentences=[]\n",
    "    return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: bible_web_data.txt\n",
      "Reading file: commun_aff.txt\n",
      "Reading file: menfesawi.txt\n",
      "Reading file: misc_data.txt\n",
      "Reading file: poemes.txt\n",
      "Reading file: social_media.txt\n",
      "Reading file: tghat_web_data.txt\n",
      "Reading file: tig_books.txt\n",
      "Reading file: tvml_t.txt\n",
      "Reading file: tvml_v.txt\n",
      "Reading file: xlum_web_data.txt\n"
     ]
    }
   ],
   "source": [
    "list_files=os.listdir(DATA_PATH)\n",
    "content = \"\"\n",
    "for filename in list_files:\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(f\"Reading file: {filename}\")\n",
    "        with open(os.path.join(DATA_PATH, filename), 'r', encoding='utf-8') as f:\n",
    "            content += f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the parameters of the GPT2\n",
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 16000,  # Vocabulary size\n",
    "    \"context\": 1024,      # Context length in number of tokens\n",
    "    \"emb_dim\": 768,       # Embedding dimension\n",
    "    \"n_heads\": 12,        # Number of attention heads\n",
    "    \"n_layers\": 12,       # Number of layers\n",
    "    \"dropout_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False     # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the GPT model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the attention layer (self and multi-head attention layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Transformer block which is composed of the above layers, linear Layer, GELU, Feedforward and attnetion heads "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT (Generative pretraining models) which is composzed of multiple transformer blocks and softmax layetr at the top. \n",
    "#### It takes embedings of the tokens and their positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around the transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the tokenizer: Here we use the GPT2 tokenizer. However, we have also trained our own bpe based tokenizer for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pr401\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./tokenizers/Tig_BPE_16000\")\n",
    "# tokenizer = Tokenizer.from_file(\"tokenizer_TIG_Trial.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = \"[መልእ]\"\n",
    "tokenizer.cls_token = \"[ጀመረ]\"\n",
    "tokenizer.sep_token = \"[ከፋሊ]\"\n",
    "tokenizer.mask_token = \"[ሽፉን]\"\n",
    "tokenizer.unk_token = \"[ለየለ]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing aroung with the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_size = 'right'\n",
    "tokenizer.add_special_tokens({'pad_token': '[መልእ]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 35])\n",
      "tensor([[12532,  1756,  5733,  2531,   329,   615,   221,   646,  1783,   518,\n",
      "          7969, 13986,   355,   896,   745,  8944,  3521,   288,  1972,   606,\n",
      "          6694,  1490,  5058,   265,  5213,   198, 13030, 12298,  5072,   262,\n",
      "           402,  8177,   353,   300,   204],\n",
      "        [  239,  9499,  4970, 14472,  1678,   611,   745,   288, 15817,  6887,\n",
      "          2388,  1824,  6068,  2219,   296,  7624,  1018,   279,  6035,   377,\n",
      "          3846,   300,   204,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2]])\n",
      "tensor([[12532,  1756,  5733,  2531,   329,   615,   221,   646,  1783,   518,\n",
      "          7969, 13986,   355,   896,   745,  8944,  3521,   288,  1972,   606,\n",
      "          6694,  1490,  5058,   265,  5213,   198, 13030, 12298,  5072,   262,\n",
      "           402,  8177,   353,   300,   204],\n",
      "        [  239,  9499,  4970, 14472,  1678,   611,   745,   288, 15817,  6887,\n",
      "          2388,  1824,  6068,  2219,   296,  7624,  1018,   279,  6035,   377,\n",
      "          3846,   300,   204,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2]])\n"
     ]
    }
   ],
   "source": [
    "C = 128\n",
    "batch = []\n",
    "\n",
    "txt1 = \"ስነ ስርዓት ብሄራዊ ሓዘን፣መርድእን ዝኽርን ስውኣት ሃርበርኛታት ትግራይ ተግባራዊ ንምግባር ካብ ክልል ክሳብ ጣብያ ዝተፈላለዩ ኮሚቴታት ተጣይሾም ምይይጥ ይካየድ ከም ዘሎ እናተሓበረ እዩ።\"\n",
    "txt2 = \"ደቅና ዝወደቕሉ ዕላማ ህዝቢ ትግራይ ካብ ብርሰት ንምድሓን ስለዝኾነ መስዋእቶም ህያውን ብኽብሪ እናተዘከረ ዝነብርን እዩ።\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1,padding=True,truncation=True,max_length=C)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2,padding=True,truncation=True,max_length=14)))\n",
    "\n",
    "batch = tokenizer(\n",
    "    [txt1, txt2],\n",
    "    padding=\"max_length\",  # pad to max_length\n",
    "    truncation=True,\n",
    "    max_length=35,         # or whatever length you want\n",
    "    return_tensors=\"pt\"    # returns PyTorch tensors\n",
    ")[\"input_ids\"]\n",
    "\n",
    "print(batch.shape)\n",
    "print(batch)\n",
    "\n",
    "# batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG[\"vocab_size\"]=tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 16000,\n",
       " 'context': 1024,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'dropout_rate': 0.1,\n",
       " 'qkv_bias': False}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying the GPT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[12532,  1756,  5733,  2531,   329,   615,   221,   646,  1783,   518,\n",
      "          7969, 13986,   355,   896,   745,  8944,  3521,   288,  1972,   606,\n",
      "          6694,  1490,  5058,   265,  5213,   198, 13030, 12298,  5072,   262,\n",
      "           402,  8177,   353,   300,   204],\n",
      "        [  239,  9499,  4970, 14472,  1678,   611,   745,   288, 15817,  6887,\n",
      "          2388,  1824,  6068,  2219,   296,  7624,  1018,   279,  6035,   377,\n",
      "          3846,   300,   204,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2]])\n",
      "\n",
      "Output shape: torch.Size([2, 35, 16000])\n",
      "tensor([[[ 0.1725, -0.7187, -0.6800,  ..., -0.7375, -0.5550, -0.3602],\n",
      "         [-0.3978, -0.4335, -1.2455,  ...,  0.5939,  0.0061, -0.7532],\n",
      "         [ 1.0028, -0.3691, -0.7015,  ..., -0.7258, -1.1962, -0.2808],\n",
      "         ...,\n",
      "         [ 0.3745, -0.2554,  0.1445,  ..., -0.0289, -0.0688,  0.5678],\n",
      "         [ 0.1980,  0.3967,  0.7678,  ..., -0.1246, -0.4080, -0.4212],\n",
      "         [ 0.0474,  0.9927, -0.3053,  ..., -0.0262,  0.7011,  0.7192]],\n",
      "\n",
      "        [[-0.4493, -0.0217,  0.2564,  ..., -0.2467, -0.2824,  0.3927],\n",
      "         [-0.4119,  0.5027, -0.4208,  ...,  0.1348,  0.3374, -0.1335],\n",
      "         [ 0.4408,  0.0406, -0.3982,  ...,  0.3791,  0.0846,  0.2752],\n",
      "         ...,\n",
      "         [ 1.2760, -0.4907,  0.2659,  ...,  0.4294, -0.6130,  0.4436],\n",
      "         [ 1.0876,  0.0695,  0.0870,  ...,  0.3747, -0.5088, -0.6580],\n",
      "         [ 0.4648,  0.5035, -0.3514,  ...,  1.1161, -0.2085,  0.4053]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 35, 16000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 16000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.flatten(0,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 110,390,784\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([16000, 768])\n",
      "Output layer shape: torch.Size([16000, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory size of the model: 0.41 GB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "size_in_gb = (total_params * 4)/(1024 * 1024 * 1024)\n",
    "\n",
    "print(f\"Total memory size of the model: {size_in_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 35, 16000])\n",
      "tensor([[[ 0.3796, -0.3549, -0.2432,  ..., -0.3788, -0.2950, -0.0339],\n",
      "         [-0.8383, -1.0397, -0.6864,  ...,  0.9860, -0.4093, -0.6160],\n",
      "         [ 0.8875, -0.3274, -0.0910,  ..., -0.9472, -1.2487, -0.3357],\n",
      "         ...,\n",
      "         [ 0.4272, -0.2393,  0.6474,  ..., -0.3958, -0.0268, -0.0401],\n",
      "         [ 0.2197,  0.4848,  0.9294,  ..., -0.0233, -0.1967,  0.2135],\n",
      "         [-0.0419,  0.8995, -0.3028,  ...,  0.0717,  0.5144,  0.4318]],\n",
      "\n",
      "        [[-0.4489, -0.0767, -0.0254,  ..., -0.2010,  0.2316,  0.4263],\n",
      "         [-0.1254, -0.1688, -0.3220,  ...,  0.5243,  0.7971, -0.3397],\n",
      "         [ 0.4661, -0.0124, -0.1558,  ...,  0.6615, -0.4667, -0.8819],\n",
      "         ...,\n",
      "         [ 1.1056, -0.7179, -0.2374,  ...,  0.4384, -0.8988, -0.1041],\n",
      "         [ 0.3817,  0.1849,  0.5021,  ...,  0.5512, -0.5971, -0.4292],\n",
      "         [ 0.3433,  0.2350, -0.3063,  ...,  0.7231, -0.2712,  0.4317]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 35, 16000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing for Training \n",
    "######   Data need to be loaded \n",
    "######   A generator to check what the model can generate during training to see how it is progressing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "import torch\n",
    "\n",
    "def generate_predictions(model, input_data, top_k=3, temperature=1.0):\n",
    "  \"\"\"Generates predictions from a generative model.\n",
    "\n",
    "  Args:\n",
    "      model: The generative model to use for prediction.\n",
    "      input_data: The input data for the model (tensor).\n",
    "      top_k: The number of top probable predictions to return (optional, defaults to 3).\n",
    "      temperature: The temperature for sampling (optional, defaults to 1.0).\n",
    "\n",
    "  Returns:\n",
    "      A list containing the top predicted tokens/classes and their probabilities.\n",
    "  \"\"\"\n",
    "  # Ensure input data is a tensor\n",
    "  if not torch.is_tensor(input_data):\n",
    "    input_data = torch.tensor(input_data)\n",
    "\n",
    "  # Get logits (unnormalized outputs) from the model\n",
    "  with torch.no_grad():  # Deactivate gradient calculation for efficiency\n",
    "    logits = model(input_data)\n",
    "\n",
    "  # Apply temperature (optional)\n",
    "  if temperature != 1.0:\n",
    "    logits /= temperature\n",
    "\n",
    "  # Generate different predictions based on the provided arguments:\n",
    "  if top_k > 0:\n",
    "    # Top-k sampling\n",
    "    probs = torch.softmax(logits, dim=-1)  # Calculate probabilities\n",
    "    top_k_values, top_k_indices = torch.topk(probs, top_k, dim=-1)  # Get top k probabilities and indices\n",
    "    #print(top_k_indices)\n",
    "    #print(top_k_values)\n",
    "    predictions = []\n",
    "    for i in range(len(top_k_indices)):\n",
    "      prediction = []\n",
    "      for j in range(top_k):\n",
    "        index = top_k_indices[i][i][j].item()  # Convert tensor index to int\n",
    "        value = top_k_values[i][i][j].item()  # Convert tensor value to float\n",
    "        prediction.append((index, value))\n",
    "      predictions.append(prediction)\n",
    "    return predictions\n",
    "  else:\n",
    "    # Sample from the probability distribution\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predictions = torch.multinomial(probs, 1).squeeze().tolist()  # Sample one index and convert to list\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 16000])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.flatten(0,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4929,  4.4812, -1.6093], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "b = logits[0, -1, :]\n",
    "b[0] = -1.4929\n",
    "b[1] = 4.4812\n",
    "b[2] = -1.6093\n",
    "\n",
    "print(b[:3])\n",
    "res=torch.softmax(b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1865e-05, 4.6644e-03, 1.0562e-05,  ..., 5.6727e-05, 8.8318e-05,\n",
       "        8.1311e-05], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16000])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [557, 5091, 7505, 745, 149]\n",
      "encoded_tensor.shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"ጀጋኑ ስዉኣት ትግራይ \"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[  557,  5091,  7505,   745,   149, 10899, 15225, 10251,  6717, 11341,\n",
      "         15808]])\n",
      "Output length: 11\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG[\"context\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ጀጋኑ ስዉኣት ትግራይ  ተሪፋ ኣዐ ዝጀመር ተሽከ ወርቅን ትኣምን\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset for training a GPT model using a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files_by_size(file_paths, target_dir, max_size_mb=500, separator=\"<|endoftext|>\", fallback_encoding=\"latin1\"):\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    current_content = []\n",
    "    current_size = 0\n",
    "    file_counter = 1\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(os.path.join(DATA_PATH,file_path), \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            # Attempt to read the file with a fallback encoding\n",
    "            print(f\"Warning: UnicodeDecodeError encountered. Trying fallback encoding for {file_path}\")\n",
    "            with open(file_path, \"r\", encoding=fallback_encoding) as file:\n",
    "                content = file.read()\n",
    "        conts=content.split(\"<TITRE>\")\n",
    "        estimated_size = len(content.encode(\"utf-8\"))\n",
    "\n",
    "        for cont in conts:\n",
    "            if current_size + estimated_size > max_size_mb * 1024 * 1024:\n",
    "                target_file_path = os.path.join(target_dir, f\"combined_{file_counter}.txt\")\n",
    "                with open(target_file_path, \"w\", encoding=\"utf-8\") as target_file:\n",
    "                    target_file.write(separator.join(current_content))\n",
    "                file_counter += 1\n",
    "                current_content = [cont]\n",
    "                current_size = estimated_size\n",
    "            else:\n",
    "                current_content.append(cont)\n",
    "                current_size += estimated_size\n",
    "\n",
    "    if current_content:\n",
    "        target_file_path = os.path.join(target_dir, f\"combined_{file_counter}.txt\")\n",
    "        with open(target_file_path, \"w\", encoding=\"utf-8\") as target_file:\n",
    "            target_file.write(separator.join(current_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating lost at batch level and data loader \n",
    "##### It uses cross entropy loss\n",
    "##### The loss can be computed at batch levele and datasets level (train and validation loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    #print(logits.flatten(0, 1).shape, target_batch.flatten().shape)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of what is happening by ploting the loss and generating 10 words ginen an input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size)\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)#, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "#Decoding the token ids (taking greedy algorithm to choose the probabilities) using the tokenizer\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # Remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# Ploting loss and token that have been seen\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses, output_dir):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(output_dir / \"losses.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloader  class for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)#, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "    return text_data\n",
    "\n",
    "\n",
    "def create_dataloaders(text_data, train_ratio, batch_size, max_length, stride):\n",
    "    split_idx = int(train_ratio * len(text_data))\n",
    "    train_loader = create_dataloader(\n",
    "        text_data[:split_idx],\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = create_dataloader(\n",
    "        text_data[split_idx:],\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        drop_last=False,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101193740"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 1252695\n"
     ]
    }
   ],
   "source": [
    "lines = content.split(\"\\n\")  # or content.split(\".\") for sentences\n",
    "lines = [line for line in lines if line.strip()]  # remove empty lines\n",
    "print(f\"Total lines: {len(lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_batches(lines, tokenizer, batch_size, max_length, stride):\n",
    "    batch = []\n",
    "    for line in lines:\n",
    "        tokens = tokenizer(\n",
    "            line,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            stride=stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            padding=\"max_length\",   # ensures uniform length\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        for t in tokens[\"input_ids\"]:\n",
    "            t = t.squeeze(0)  # remove leading 1 dimension\n",
    "            batch.append(t)  # remove (1, max_length) -> (max_length)\n",
    "            if len(batch) == batch_size:\n",
    "                yield torch.stack(batch)  # safe now, all same size\n",
    "                batch = []\n",
    "    if batch:\n",
    "        yield torch.stack(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_batches(lines, tokenizer, batch_size, max_length, stride):\n",
    "    batch = []\n",
    "    for line in lines:\n",
    "        tokens = tokenizer(\n",
    "            line,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",   # <-- pad all sequences to max_length\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch.append(tokens[\"input_ids\"].squeeze(0))  # shape: [seq_len]\n",
    "        if len(batch) == batch_size:\n",
    "            yield torch.stack(batch)  # shape: [batch_size, seq_len]\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield torch.stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "class TokenDataset(IterableDataset):\n",
    "    def __init__(self, lines, tokenizer, batch_size, max_length, stride):\n",
    "        self.lines = lines\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(tokenized_batches(\n",
    "            self.lines,\n",
    "            self.tokenizer,\n",
    "            self.batch_size,\n",
    "            self.max_length,\n",
    "            self.stride\n",
    "        ))\n",
    "\n",
    "train_size = int(0.9 * len(lines))\n",
    "train_dataset = TokenDataset(lines[:train_size], tokenizer, batch_size=16, max_length=512, stride=256)\n",
    "val_dataset = TokenDataset(lines[train_size:], tokenizer, batch_size=16, max_length=512, stride=256)\n",
    "\n",
    "train_loader = DataLoader(train_dataset)\n",
    "val_loader = DataLoader(val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, device, n_epochs,\n",
    "                eval_freq, eval_iter, print_sample_iter, start_context,\n",
    "                output_dir, save_ckpt_freq,\n",
    "                train_loader, val_loader):\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(n_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{n_epochs} — Training ...\")\n",
    "            model.train()\n",
    "\n",
    "            for input_batch in train_loader:\n",
    "                input_batch = input_batch.to(device)  # remove any extra dimension\n",
    "                target_batch = input_batch.clone()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                tokens_seen += input_batch.numel()\n",
    "                global_step += 1\n",
    "\n",
    "                # Evaluation step\n",
    "                if global_step % eval_freq == 0:\n",
    "                    train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                    train_losses.append(train_loss)\n",
    "                    val_losses.append(val_loss)\n",
    "                    track_tokens_seen.append(tokens_seen)\n",
    "                    print(f\"Ep {epoch+1} (Step {global_step}): \"\n",
    "                          f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "                # Generate text passage occasionally\n",
    "                if global_step % print_sample_iter == 0:\n",
    "                    generate_and_print(model, train_loader.dataset.tokenizer, device, start_context)\n",
    "\n",
    "                # Save model checkpoint periodically\n",
    "                if global_step % save_ckpt_freq == 0:\n",
    "                    file_name = output_dir / f\"model_pg_{global_step}.pth\"\n",
    "                    torch.save(model.state_dict(), file_name)\n",
    "                    print(f\"Saved checkpoint: {file_name}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Forcefully stopped. Saving interrupted checkpoint...\")\n",
    "        file_name = output_dir / f\"model_pg_{global_step}_interrupted.pth\"\n",
    "        torch.save(model.state_dict(), file_name)\n",
    "        print(f\"Saved: {file_name}\")\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=1\n",
    "print_sample_iter=500\n",
    "eval_freq=50\n",
    "save_ckpt_freq=100000\n",
    "lr=5e-4\n",
    "batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1 — Training ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m output_dir = Path(\u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m output_dir.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_losses, val_losses, tokens_seen = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_sample_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_sample_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_ckpt_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_ckpt_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m epochs_tensor = torch.linspace(\u001b[32m0\u001b[39m, n_epochs, \u001b[38;5;28mlen\u001b[39m(train_losses))\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdebug\u001b[39m\u001b[33m\"\u001b[39m, epochs_tensor)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, optimizer, device, n_epochs, eval_freq, eval_iter, print_sample_iter, start_context, output_dir, save_ckpt_freq, train_loader, val_loader)\u001b[39m\n\u001b[32m     18\u001b[39m target_batch = input_batch.clone()\n\u001b[32m     20\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m loss = \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m loss.backward()\n\u001b[32m     24\u001b[39m optimizer.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcalc_loss_batch\u001b[39m\u001b[34m(input_batch, target_batch, model, device)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_loss_batch\u001b[39m(input_batch, target_batch, model, device):\n\u001b[32m      2\u001b[39m     input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m#print(logits.flatten(0, 1).shape, target_batch.flatten().shape)\u001b[39;00m\n\u001b[32m      5\u001b[39m     loss = torch.nn.functional.cross_entropy(logits.flatten(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m), target_batch.flatten())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, in_idx)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_idx):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     batch_size, seq_len = in_idx.shape\n\u001b[32m     18\u001b[39m     tok_embeds = \u001b[38;5;28mself\u001b[39m.tok_emb(in_idx)\n\u001b[32m     19\u001b[39m     pos_embeds = \u001b[38;5;28mself\u001b[39m.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "\n",
    "output_dir = Path(\"results\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    n_epochs=n_epochs,\n",
    "    eval_freq=eval_freq,\n",
    "    eval_iter=1,\n",
    "    print_sample_iter=print_sample_iter,\n",
    "    output_dir=output_dir,\n",
    "    save_ckpt_freq=save_ckpt_freq,\n",
    "    start_context=start_context,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader\n",
    ")\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, n_epochs, len(train_losses))\n",
    "\n",
    "print(\"debug\", epochs_tensor)\n",
    "#print(tokens_seen)\n",
    "#print(train_losses, val_losses)\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, output_dir)\n",
    "\n",
    "torch.save(model.state_dict(), output_dir / \"model_Tig_gpt.pth\")\n",
    "print(f\"Maximum GPU memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc0c509f510>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"results/model_pg_final.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedModel.__init__() missing 1 required positional argument: 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[292], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model1\u001b[38;5;241m=\u001b[39m\u001b[43mGPTModel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGPT2Config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model1\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m model1\u001b[38;5;241m.\u001b[39mfrom_pretrained(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/model_pg_final.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[291], line 29\u001b[0m, in \u001b[0;36mGPTModel1.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_emb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m], cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: PreTrainedModel.__init__() missing 1 required positional argument: 'config'"
     ]
    }
   ],
   "source": [
    "model1=GPTModel1(GPT2Config)\n",
    "model1.to(device)\n",
    "model1.from_pretrained(torch.load(\"results/model_pg_final.pth\"))\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"results/pytorch_model_gpt2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"J'ai cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'ai cancer du sein. <TITRE> Le rôle du pharmacien d'officine dans la prise en charge des patients atteints de cancer du sein. Sein. Conseil à l'\n"
     ]
    }
   ],
   "source": [
    "generate_and_print(model,tokenizer=tokenizer,device=device,start_context=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'ai cancer du sein. \n"
     ]
    }
   ],
   "source": [
    "encoded = text_to_token_ids(\"J'ai cancer\", tokenizer=tokenizer).to(device)\n",
    "with torch.no_grad():\n",
    "    token_ids = generate_text(model=model,\n",
    "                            idx=encoded,\n",
    "                            max_new_tokens=5, context_size=1024)\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=generate_predictions(model, input_data=encoded, top_k=3, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predi[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predi[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puting some temperature and top k predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'ai cancer\n",
      ". --> 0.051486603915691376\n",
      ", --> 0.047721248120069504\n",
      " à --> 0.017686987295746803\n",
      "J'ai cancer à\n"
     ]
    }
   ],
   "source": [
    "input_text=\"J'ai cancer\"\n",
    "encoded = text_to_token_ids(input_text, tokenizer=tokenizer).to(device)\n",
    "predi=generate_predictions(model, input_data=encoded, top_k=3, temperature=1.0)\n",
    "predi=torch.Tensor(predi)\n",
    "print(input_text)\n",
    "for i in range(len(predi[0])):\n",
    "    token_ids=[int(predi[0][i][0].item())]\n",
    "    score=predi[0][i][1].item()\n",
    "    #decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    decoded_text = tokenizer.decode(token_ids=token_ids)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"),\"-->\",score)\n",
    "input_text=input_text+decoded_text\n",
    "print(input_text)\n",
    "encoded = text_to_token_ids(input_text, tokenizer=tokenizer).to(device)\n",
    "predi=generate_predictions(model, input_data=encoded, top_k=3, temperature=1.0)\n",
    "predi=torch.Tensor(predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top k predictions:\n",
      "Token: du, Probability score: 0.17419902980327606\n",
      "Token: ., Probability score: 0.14047178626060486\n",
      "Token: 2014, Probability score: 0.05693987011909485\n"
     ]
    }
   ],
   "source": [
    "def predict(model, prompt, top_k=3):\n",
    "    \n",
    "    # Encode the prompt text and convert to input tensors\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    #input_ids.to(device)\n",
    "    # Get logits of the last token\n",
    "    with torch.no_grad():  # No need to compute gradients\n",
    "        outputs = model(input_ids).to(device)\n",
    "        predictions = outputs[:, -1, :]\n",
    "\n",
    "    #print(predictions)\n",
    "    # Get the top k tokens and their log probabilities\n",
    "    predictions=torch.softmax(outputs[:, -1, :],dim=-1)\n",
    "    top_k_values, top_k_indices = torch.topk(predictions, top_k)\n",
    "    top_k_log_probs = top_k_values[0].tolist()  # Convert to list\n",
    "    #top_k_probs = torch.exp(top_k_values[0]).tolist()\n",
    "    top_k_tokens = [tokenizer.decode(index).strip() for index in top_k_indices[0].tolist()]\n",
    "\n",
    "    #print(top_k_values[0])\n",
    "    # Combine tokens with their log probabilities\n",
    "    top_k_predictions = list(zip(top_k_tokens,top_k_log_probs))\n",
    "\n",
    "    return top_k_predictions\n",
    "\n",
    "\n",
    "top_k_predictions = predict(model=model,prompt=prompt, top_k=3)\n",
    "\n",
    "print(\"Top k predictions:\")\n",
    "for token, log_prob in top_k_predictions:\n",
    "    print(f\"Token: {token}, Probability score: {log_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "#input_ids.to(device)\n",
    "# Get logits of the last token\n",
    "with torch.no_grad():  # No need to compute gradients\n",
    "    outputs = model(input_ids).to(device)\n",
    "    predictions = outputs[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1263,  1.0577, -3.0049,  ..., -4.9220, -9.1450, -8.5778]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap=F.log_softmax(output,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=torch.softmax(output[:-1:],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[[0.0070, 0.0048, 0.0047],\n",
       "         [0.0045, 0.0045, 0.0044],\n",
       "         [0.0046, 0.0044, 0.0040],\n",
       "         [0.0051, 0.0036, 0.0035]]], grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[[536, 637, 110],\n",
       "         [548, 624, 692],\n",
       "         [584, 160, 428],\n",
       "         [ 33, 189, 544]]]))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(aa,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[-4.7007, -4.8994, -5.1059],\n",
       "        [-5.3264, -5.3997, -5.5259],\n",
       "        [-5.3318, -5.3478, -5.4549],\n",
       "        [-5.6147, -5.6250, -5.7012]], grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[338, 195, 344],\n",
       "        [ 42, 191,  80],\n",
       "        [338, 199, 691],\n",
       "        [ 55, 131, 375]]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(F.log_softmax(o,dim=-1),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model,input_text, beam_width=3, max_length=10):\n",
    "\n",
    "    beam=[(input_text,0.0)]\n",
    "\n",
    "    for i in range(max_length):\n",
    "        candidates=[]\n",
    "\n",
    "        for seq, score in beam:\n",
    "            print(seq,\":\",score)\n",
    "            predictions=predict(model=model,prompt=seq,top_k=beam_width)\n",
    "\n",
    "            for word, prob in predictions:\n",
    "\n",
    "                new_seq=seq+\" \"+word\n",
    "                new_score=score+prob\n",
    "\n",
    "                candidates.append((new_seq,new_score))\n",
    "    \n",
    "        beam=sorted(candidates,key=lambda x:x[1], reverse=True)[:beam_width]\n",
    "\n",
    "    return beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'ai cancer : 0.0\n",
      "J'ai cancer du : 0.08337871730327606\n",
      "J'ai cancer 2014 : 0.06458677351474762\n",
      "J'ai cancer . : 0.05127895623445511\n",
      "J'ai cancer du se : 0.6106284111738205\n",
      "J'ai cancer .  : 0.4901657775044441\n",
      "J'ai cancer 2014 . : 0.4407401531934738\n",
      "J'ai cancer du se in : 1.6047788709402084\n",
      "J'ai cancer 2014 .  : 0.7916611582040787\n",
      "J'ai cancer .   : 0.7541298344731331\n",
      "J'ai cancer du se in clus : 2.177734300494194\n",
      "J'ai cancer du se in situ : 1.6499392613768578\n",
      "J'ai cancer du se in ut : 1.623113626614213\n",
      "J'ai cancer du se in clus ant : 2.422809660434723\n",
      "J'ai cancer du se in clus . : 2.2979949191212654\n",
      "J'ai cancer du se in situ ant : 2.2666399106383324\n",
      "J'ai cancer du se in clus ant é : 3.156162738800049\n",
      "J'ai cancer du se in clus .  : 2.6820903792977333\n",
      "J'ai cancer du se in situ ant é : 2.605286620557308\n",
      "J'ai cancer du se in clus ant é vol : 3.356650948524475\n",
      "J'ai cancer du se in clus ant é lev : 3.3375744968652725\n",
      "J'ai cancer du se in clus ant é ta : 3.238456517457962\n",
      "J'ai cancer du se in clus ant é vol ont : 4.230773568153381\n",
      "J'ai cancer du se in clus ant é lev é : 3.618282690644264\n",
      "J'ai cancer du se in clus ant é lev ier : 3.478270396590233\n",
      "J'ai cancer du se in clus ant é vol ont  : 4.884672462940216\n",
      "J'ai cancer du se in clus ant é vol ont e : 4.270860910415649\n",
      "J'ai cancer du se in clus ant é vol ont é : 4.250544892624021\n"
     ]
    }
   ],
   "source": [
    "beams=beam_search(model=model,input_text=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: J'ai cancer du se in clus .  L ' object if, Score: 0.5472464963793755\n",
      "Prediction: J'ai cancer du se in clus .  N ous , , Score: 0.47964938059449197\n",
      "Prediction: J'ai cancer du se in clus .  L ' é val, Score: 0.46785239726305006\n"
     ]
    }
   ],
   "source": [
    "for seq, score in beams:\n",
    "    print(f\"Prediction: {seq}, Score: {score/10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context': 1024,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'dropout_rate': 0.1,\n",
       " 'qkv_bias': False}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_args={'gpt2':dict(n_layer=12, n_head=12, n_embd=768)}\n",
    "config_args['vocab_size'] = GPT_CONFIG[\"vocab_size\"] # always 50257 for GPT model checkpoints\n",
    "config_args['block_size'] = GPT_CONFIG[\"context\"] # always 1024 for GPT model checkpoints\n",
    "config_args['bias'] = True\n",
    "config_args[\"emb_dim\"]=GPT_CONFIG[\"emb_dim\"]\n",
    "config_args['dropout']=GPT_CONFIG[\"dropout_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=GPT2Config.from_pretrained('gpt2',output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"results/model_pg_final.pth\",map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "def convert_local_gpt2_to_hf(model_path, hf_model_name):\n",
    "  \"\"\"Converts a locally saved PyTorch GPT2 model to a Hugging Face Transformers model.\n",
    "\n",
    "  Args:\n",
    "      model_path: Path to the directory containing the saved PyTorch model weights (.pt file).\n",
    "      hf_model_name: Name for the Hugging Face Transformers model (e.g., \"converted_gpt2\").\n",
    "  \"\"\"\n",
    "\n",
    "  # Load model data\n",
    "  loaded_data = torch.load(f\"{model_path}/pytorch_model.pt\")\n",
    "\n",
    "  # Access the state dictionary (if it exists)\n",
    "  model_state_dict = loaded_data.get('model_state_dict', None)\n",
    "\n",
    "  if model_state_dict is not None:\n",
    "      # Get model config (assuming it's a GPT2 model)\n",
    "      config=GPT2Config(\n",
    "        context=GPT_CONFIG[\"context\"],\n",
    "        emb_dim=GPT_CONFIG[\"emb_dim\"],\n",
    "        dropout_rate=0.1,\n",
    "        qkv_bias=False)\n",
    "      # Pretrained config as a base\n",
    "\n",
    "      # Create new Hugging Face Transformers model\n",
    "      hf_model = GPT2LMHeadModel(config)\n",
    "\n",
    "      # Load state dict into Hugging Face model (potentially with key remapping)\n",
    "      hf_model.load_state_dict(model_state_dict, strict=False)\n",
    "\n",
    "      # Save Hugging Face Transformers model\n",
    "      hf_model.save_pretrained(hf_model_name)\n",
    "      print(f\"Converted PyTorch model to Hugging Face Transformers model at: {hf_model_name}\")\n",
    "  else:\n",
    "      print(\"Error: Could not find 'model_state_dict' in loaded data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = torch.load(pytorch_model_path)\n",
    "\n",
    "# Access the state dictionary (if it exists)\n",
    "model_state_dict = loaded_data.get('model_state_dict', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "pytorch_model_path = \"/home/aberhe/Projects/SANTAL/Course/notebook/results/model_pg_final.pth\"\n",
    "hf_model_name = \"converted_gpt2\"\n",
    "#convert_gpt2_pth_to_hf(pytorch_model_path, hf_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the custom model's state dictionary\n",
    "state_dict = torch.load(pytorch_model_path, map_location='cpu')\n",
    "\n",
    "# Load the configuration of GPT-2. Adjust this if you've used a different configuration\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize the Hugging Face model with this configuration\n",
    "model_hf = GPT2LMHeadModel(config)\n",
    "\n",
    "# Update the Hugging Face model with your custom weights\n",
    "# Ensure the keys in state_dict match those expected by GPT2LMHeadModel; adjustments might be necessary\n",
    "model_hf.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Save the Hugging Face model in its expected format\n",
    "model_hf.save_pretrained('./my_huggingface_gpt2_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_huggingface_gpt2_model/tokenizer_config.json',\n",
       " './my_huggingface_gpt2_model/special_tokens_map.json',\n",
       " './my_huggingface_gpt2_model/vocab.json',\n",
       " './my_huggingface_gpt2_model/merges.txt',\n",
       " './my_huggingface_gpt2_model/added_tokens.json')"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.save_pretrained('./my_huggingface_gpt2_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'ai cancerhillaryhillaryhillary Fear Fear Fear\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained('my_huggingface_gpt2_model')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('my_huggingface_gpt2_model')\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize input while ensuring padding and attention mask are correctly used\n",
    "input_text = \"J'ai cancer\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "# Generate text with specified pad_token_id and attention_mask\n",
    "outputs = model_hf.generate(input_ids, attention_mask=attention_mask, pad_token_id=tokenizer.pad_token_id, max_length=10)\n",
    "\n",
    "# Decode and print the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "  model_state_dict = torch.load(pytorch_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[281], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_state_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "model_state_dict.state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=GPT2Config(\n",
    "        context=GPT_CONFIG[\"context\"],\n",
    "        emb_dim=GPT_CONFIG[\"emb_dim\"],\n",
    "        dropout_rate=0.1,\n",
    "        qkv_bias=False\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"context\": 1024,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"emb_dim\": 768,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"qkv_bias\": false,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context': 1024,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'dropout_rate': 0.1,\n",
       " 'qkv_bias': False}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_model_to_huggingface_checkpoint(model_name, new_name = None):\n",
    "    \"\"\"Creates the Huggingface save, easily used for other purposes\n",
    "    The files are saved in the path chosen in the config file.\n",
    "\n",
    "    Args:\n",
    "        checkpoint (str): name of the checkpoint\n",
    "        model_name (str): name of the future huggingface save\n",
    "    \"\"\"\n",
    "\n",
    "    config=GPT2Config.from_pretrained('gpt2')\n",
    "    model_hf = GPT2LMHeadModel(config)\n",
    "    \n",
    "    state_dict = torch.load(model_name, map_location='cpu')\n",
    "    \"\"\"\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k#[7:] # remove module.\n",
    "        print(name)\n",
    "        new_state_dict[name] = v\n",
    "    # load params\n",
    "    model_hf.load_state_dict(state_dict=state_dict)\n",
    "    \"\"\"\n",
    "    model_hf.load_state_dict(state_dict, strict=False)\n",
    "    if new_name==None:\n",
    "        model_hf.save_pretrained(\"my_huggingface_gpt2_model\")\n",
    "    else:\n",
    "        model_hf.save_pretrained(new_name)\n",
    "    \n",
    "    return model_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytorch_model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_hf\u001b[38;5;241m=\u001b[39mpytorch_model_to_huggingface_checkpoint(model_name\u001b[38;5;241m=\u001b[39m\u001b[43mpytorch_model_path\u001b[49m,new_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2_hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pytorch_model_path' is not defined"
     ]
    }
   ],
   "source": [
    "model_hf=pytorch_model_to_huggingface_checkpoint(model_name=pytorch_model_path,new_name=\"gpt2_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'ai cancer vivo vivo vivo vivo vivo vivo\n"
     ]
    }
   ],
   "source": [
    "input_text = \"J'ai cancer\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "# Generate text with specified pad_token_id and attention_mask\n",
    "outputs = model_hf.generate(input_ids, attention_mask=attention_mask, pad_token_id=tokenizer.pad_token_id, max_length=10)\n",
    "\n",
    "# Decode and print the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM, RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.31.0\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"use_cache\": True,\n",
    "  \"vocab_size\": 1600\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "            vocab_size = args[\"vocab_size\"],\n",
    "            max_position_embeddings = args[\"max_position_embeddings\"],\n",
    "            hidden_size = args[\"hidden_size\"],\n",
    "            num_attention_heads = args[\"num_attention_heads\"],\n",
    "            num_hidden_layers = args[\"num_hidden_layers\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=RobertaForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(1600, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=1600, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
